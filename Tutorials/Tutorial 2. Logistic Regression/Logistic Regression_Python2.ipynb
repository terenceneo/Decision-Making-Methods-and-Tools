{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 Introduction to Logistic Regression\n",
    "\n",
    "#### Version: Python 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal:\n",
    "\n",
    "In this notebook, we will explore logistic regression using:\n",
    "* Gradient ascent method (because you cannot get closed-form solutions)\n",
    "* Open-source package: `scikit-learn`\n",
    "\n",
    "For the gradient descent method, you will:\n",
    "* Use numpy to write functions\n",
    "* Write a likelihood function\n",
    "* Write a derivative function\n",
    "* Write an output function\n",
    "* Write a gradient ascent function\n",
    "* Add a constant column of 1's as intercept term\n",
    "* Use the gradient ascent function to get regression estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Summary of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture class, we know that a typical logistic regression model of *N* observations and *p* predictors:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Logit}(P(y_{i}=1)) &= log(\\frac{P(y_{i}=1)}{1-P(y_{i}=1)}) \\\\\n",
    "& = \\beta_0 + \\sum_{j=1}^p x_j \\beta_j \\\\\n",
    "& = X\\beta\n",
    "\\end{align}\n",
    "\n",
    "Rewrite the model and we can get this function:\n",
    "\n",
    "$$ P(y_{i}=1) = \\frac{e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}} $$\n",
    "\n",
    "\n",
    "We aim to **Maximize** the (log-)likelihood function:\n",
    "\n",
    "$$ l(\\beta) = \\prod_{i=1}^{N}[\\frac{e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}]^{y_i}[\\frac{1}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}]^{(1-y_i)} $$\n",
    "Or\n",
    "$$ ll(\\beta) = \\sum_{i=1}^{N}[-log(1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j})+y_{i}(\\beta_{0}+\\sum_{j=1}^{p}x_j\\beta_j)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us **Maximize** the log-likelihood function $ll(\\beta)$ and get derivative with respect to $\\beta_j$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial ll(\\beta)}{\\partial\\beta_j} & = -\\sum_{i=1}^{N}\\frac{e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}x_{ij}+\\sum_{i=1}^{N}y_{i}x_{ij} \\\\\n",
    "& = \\sum_{i=1}^{N}(y_{i}-\\frac{e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}})x_{ij} \\\\\n",
    "& = \\sum_{i=1}^{N}(y_{i}-P(y_{i}=1))x_{ij} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the predicted score $P(y_{i}=1)$ is calculated as $\\frac{e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}=\\frac{1}{1+e^{-(\\beta_{0}+\\sum_{j=1}^{p}x_{j}\\beta_{j})}}$ <br/>\n",
    "\n",
    "Then we can write a predicted score function first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(feature_matrix, weight_vector):\n",
    "    '''This function is used to calculate predicted probability or score.\n",
    "    \n",
    "    Inputs:\n",
    "    1) feature_matrix: A matrix of selected features;\n",
    "    2) weight_vector: A vector of coefficients for selected features;\n",
    "    \n",
    "    Outputs:\n",
    "    1) score: A vector of predicted probabilities or scores   \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    score = 1/(1+np.exp(-np.dot(feature_matrix, weight_vector)))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a test\n",
    "features = np.array([[2.0, 1.5], [4.8, 5.2]])\n",
    "weights = np.array([1.0, 1.0])\n",
    "weights = np.ones((2, 1))\n",
    "score(features, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Gradient Ascent\n",
    "### 2.1 Computing Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we compute the derivatives of the log-likelihood function? <br/>\n",
    "\n",
    "We already know that the derivative with respect to $\\beta_j$ is: <br/>\n",
    "\\begin{align*}\n",
    "\\frac{\\partial ll(\\beta)}{\\partial\\beta_j} & = \\sum_{i=1}^{N}(y_{i}-P(y_{i}=1))x_{ij} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(feature_matrix, error_vector):\n",
    "    '''This function is used to calculate the derivatives for features.\n",
    "    Inputs:\n",
    "    1) feature_matrix: Data matrix of features (j = 0,...,p)\n",
    "    2) error_vector: A vector of errors of N observations    \n",
    "    \n",
    "    Outputs:\n",
    "    1) derive: Derivative for this feature j    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    derive = np.dot(feature_matrix.T, error_vector)\n",
    "    \n",
    "    return derive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a test\n",
    "features = np.array([[2.0], [4.8]])\n",
    "weights = np.ones((1, 1))\n",
    "true_outputs = np.array([[1], [0]])\n",
    "predict_score = score(features, weights)\n",
    "errors = true_outputs - predict_score\n",
    "\n",
    "print \"True output is: \"\n",
    "print true_outputs\n",
    "print \"Predicted score is: \"\n",
    "print predict_score\n",
    "print \"Error is: \"\n",
    "print errors\n",
    "print \"Derivative is: \"\n",
    "print derivative(features, errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Computing Log-likehood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture class, we know the log likelihood function is computed as:\n",
    "\n",
    "\\begin{align*}\n",
    "ll(\\beta) & = \\sum_{i=1}^{N}[-log(1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j})+y_{i}(\\beta_{0}+\\sum_{j=1}^{p}x_j\\beta_j)] \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(feature_matrix, weight_vector, output_vector):\n",
    "    '''This function is used to calculate log-likelihood value of given features, output and coefficients\n",
    "    Inputs:\n",
    "    1) feature_matrix: A matrix of selected features\n",
    "    2) weight_vector: A vector of coefficients\n",
    "    3) output_vector: A vector of true outputs\n",
    "    \n",
    "    Outputs:\n",
    "    1) ll: Log-likelihood value\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Linear term: Xbeta\n",
    "    X_beta = np.dot(feature_matrix, weight_vector)\n",
    "    \n",
    "    # Sum term: The term in the sum_up bracket\n",
    "    sum_term = -np.log(1+np.exp(X_beta)) + output_vector*X_beta\n",
    "    \n",
    "    # Get log-likelihood\n",
    "    ll = np.sum(sum_term)    \n",
    "    \n",
    "    return ll       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Procedures for Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0: Initialize $ \\beta = (\\beta_{0},\\dots,\\beta_{p}) $; <br/>\n",
    "Step 1: Calculate $ \\beta_{j} \\leftarrow \\beta_{j} + Stepsize\\times(\\sum_{i=1}^{N}(y_{i}-P(y_{i}=1))x_{ij}) $, for j = 1,...,p; <br/>\n",
    "Step 2: If not converged, go back to Step 1; <br/>\n",
    "Step 3: Get $ \\beta $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_GA(initial_weights, feature_matrix, true_output, step_size, tolerance, n_iter):\n",
    "    '''This function is used to iteratively calculate coefficients for logistic regression model using Gradient Aescent.\n",
    "    Inputs:\n",
    "    1) initial_weights: Initial regression coefficients\n",
    "    2) feature_matrix: A matrix of selected features\n",
    "    3) true_output: A vector of true outputs -> [0, 1]\n",
    "    4) step_size: Size of step for each iteration of gradient search\n",
    "    5) tolerance: Indicate converging condition\n",
    "    6) n_iter: Maximum number of iterations\n",
    "    \n",
    "    Outputs:\n",
    "    1) weights: Estimated coefficients.    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    weights = np.array(initial_weights, dtype=np.float64)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Calculate predicted scores, or probabilitis of 1's\n",
    "        prediction = score(feature_matrix, weights)\n",
    "        \n",
    "        # Calculate errors\n",
    "        error = true_output - prediction\n",
    "        \n",
    "        # Calculating derivatives for weights\n",
    "        derivative_vector = derivative(feature_matrix, error) \n",
    "        \n",
    "        # Updating weights\n",
    "        weights += step_size * derivative_vector        \n",
    "        \n",
    "        # Converging conditions: L2 norm for derivatives\n",
    "        sum_squared_gradient = np.sum(np.power(derivative_vector,2))\n",
    "        \n",
    "        # Verify whether converging early and Report Log-likelihood values            \n",
    "        if np.sqrt(sum_squared_gradient) < tolerance: \n",
    "            # Calculate log-likelihood\n",
    "            loglike = log_likelihood(feature_matrix, weights, true_output)\n",
    "            \n",
    "            # Print out log-likelihood\n",
    "            print 'Iteration %10d: Log-likelihood = %.4f' % (i, loglike)\n",
    "            \n",
    "            return weights\n",
    "        \n",
    "        elif (i<=20) or (i<=100 and i%10==0) or (i<=1000 and i%100==0) or (i==n_iter):            \n",
    "            # Calculate log-likelihood\n",
    "            loglike = log_likelihood(feature_matrix, weights, true_output)\n",
    "            \n",
    "            # Print out log-likelihood\n",
    "            print 'Iteration %10d: Log-likelihood = %.4f' % (i, loglike)\n",
    "            \n",
    "    \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Convert Pandas DataFrame to Numpy Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to: <br/>\n",
    "1. Convert Pandas DataFrame into a Numpy Array/Matrix to do internal calculations;\n",
    "2. Augment this Array/Matrix by adding 1's column in the first column, in order to calculate the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(df, feature_names, output_name):\n",
    "    '''This function is used to convert pandas dataframe to numpy array/matrix, and augment it with 1's column as intercept.\n",
    "    Inputs:\n",
    "    1) df: Original data in the format of pandas dataframe\n",
    "    2) feature_names: A list of names of selected features\n",
    "    3) output_name: Name of selected outputs\n",
    "    \n",
    "    Outputs:\n",
    "    1) feature_matrix: Augmented feature matrix\n",
    "    2) output_vector: A vector of true outputs    \n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    # Feature names of Augmented matrix \n",
    "    augment_feature_names = ['intercept'] + feature_names\n",
    "    \n",
    "    # Augmented feature matrix by adding constant 1's as intercept term, and reorder the feature matrix\n",
    "    df['intercept'] = 1 \n",
    "    feature_matrix = df[augment_feature_names]\n",
    "    n, k = feature_matrix.shape # n: number of observations; k: number of weights\n",
    "    \n",
    "    # Convert selected feature matrix and output vector to Numpy Array\n",
    "    feature_matrix = feature_matrix.values.reshape((n, k))\n",
    "    output_vector = df[output_name].values.reshape((n, 1))\n",
    "\n",
    "    return (feature_matrix, output_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Load Dataset and Conduct Estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset can be obtained from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Iris).\n",
    "\n",
    "### 4.1 Attribute Information:\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. sepal length in cm \n",
    "2. sepal width in cm \n",
    "3. petal length in cm \n",
    "4. petal width in cm \n",
    "5. class: <br/>\n",
    "-- Iris Setosa <br/>\n",
    "-- Iris Versicolour <br/>\n",
    "-- Iris Virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose: 1:class='Iris-setosa'; 0:otherwise\n",
    "### It means we want to predict whether class is 'Iris-setosa' (Y=1) or not (Y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset into Python Pandas DataFrame\n",
    "filepath = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "colnames = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "data = pd.read_csv(filepath, header=None, names=colnames, dtype={'sepal_length':np.float64, 'sepal_width':np.float64, 'petal_length':np.float64, 'petal_width':np.float64})\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna() # Drop null values\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output dummy\n",
    "data['Iris_setosa'] = data['class'].map(lambda x: 1 if x=='Iris-setosa' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and output\n",
    "feature_list =  ['sepal_width', 'petal_length']\n",
    "output_label = 'Iris_setosa'\n",
    "\n",
    "features = convert_data(data, feature_list, output_label)[0]\n",
    "output = convert_data(data, feature_list, output_label)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up initial parameters and try gradient ascent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "initial_weights = np.ones((len(feature_list)+1, 1))\n",
    "step_size = 7e-5 \n",
    "tolerance = 2e-9\n",
    "iteration = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try estimation with gradient descent\n",
    "coefficients = Logistic_GA(initial_weights, features, output, step_size, tolerance, iteration)\n",
    "print '\\nIntercept ','X1', 'X2'\n",
    "print coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try different `initial_weights`, `step_size`, `tolerance`, or `iteration`.<br/>\n",
    "You can also use different features to run logistic regression model. <br/>\n",
    "Large step size may make gradient search fluctuate too much, which is more difficult to converge. <br/>\n",
    "Small tolerance may increase the number of iterations, but will result in more accurate estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Logistic Classifier\n",
    "\n",
    "In this section, you will:\n",
    "* Use `scikit-learn` to run logistic regression model\n",
    "* Do train-test split\n",
    "* Understand ROC curve and AUC\n",
    "* Understand confusion matrix (TP, FP, TN and FN), and performance measures (e.g., accuracy, precision, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Scikit-Learn\n",
    "The package `scikit-learn` can be found at http://scikit-learn.org/stable/index.html. <br/>\n",
    "Please install the package first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "X = data[['sepal_width', 'petal_length']]\n",
    "y = data['Iris_setosa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do train-test split: 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on train data: Using L2-regularization\n",
    "lr = LogisticRegression(fit_intercept=True, max_iter=1000, tol=2e-9, penalty='l2', C=100, random_state=0)\n",
    "lr.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients\n",
    "print lr.intercept_, lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict outputs for test data\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print cm\n",
    "print TN, FP, FN, TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Get predicted scores Pr(y=1): Used as thresholds for calculating TP Rate and FP Rate\n",
    "# lr.classes_\n",
    "score = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, score) # fpr: FP Rate, tpr: TP Rate, thresholds: Pr(y=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about Logistic Regression can be found at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using K-Fold Cross Validation to Do Model Evaluation\n",
    "\n",
    "#### K-Fold Cross Validation:\n",
    "\n",
    "<img src='http://www.scielo.br/img/revistas/jmoea/v16n3//2179-1074-jmoea-16-03-0628-gf04.jpg' width='500'>\n",
    "\n",
    "K-Fold cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a standard binary classification dataset from the UCI machine learning repository \"Haberman's Survival Data Set\" (https://archive.ics.uci.edu/ml/datasets/Haberman's+Survival). The dataset has one binary output and three numeric input variables of varying scales. The details about the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Haberman's+Survival)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset: \n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\"\n",
    "names = ['age', 'year_operation', 'positive_detected', 'survive']\n",
    "dataframe = pd.read_csv(url, names=names)\n",
    "dataframe['survive'] = dataframe['survive'].map(lambda x:1 if x==1 else 0)  # =1, survive after 5 years; =0, die within 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive analysis\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and output\n",
    "features = ['age', 'year_operation', 'positive_detected']\n",
    "output = 'survive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Do K-fold Cross Validation\n",
    "* Set `n_splits=5`: 5-fold cross validation\n",
    "* Set `random_state` to `12345`\n",
    "\n",
    "Hint: Use `KFold` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 5-fold cross validation\n",
    "kf = \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Create Multiple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model 1: Logistic regression with L2 regularization\n",
    "* Model 2: Logistic regression with L1 regularization (Let's use the same parameter values as Model 1, except `penalty`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic regression with L2 regularization\n",
    "model_1 = LogisticRegression(fit_intercept=True, max_iter=1000, tol=2e-9, penalty='l2', C=100, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Logistic regression with L1 regularization (Let's use the same parameter values as Model 1, except penalty)\n",
    "model_2 = \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Conduct K-fold Cross-Validation on These Models\n",
    "\n",
    "Hint: Use `cross_val_score` function\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on Model 1\n",
    "cv_model_1 = cross_val_score(model_1, # Cross-validation on model_1\n",
    "                             dataframe[features], # Feature matrix\n",
    "                             dataframe[output], # Output vector\n",
    "                             cv=kf, # Cross-validation technique\n",
    "                             scoring='accuracy' # Model performance metrics: accuracy\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on Model 2\n",
    "cv_model_2 =\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Report and Evaluate Mean Accuracy of These Models\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/1*J2B_bcbd1-s1kpWOu_FZrg.png' width='700'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report average cross-validation accuracy of Model 1\n",
    "\n",
    "\n",
    "# Report average cross-validation accuracy of Model 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. Based on the cross-validation accuacy, which model do your prefer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 References\n",
    "\n",
    "[1] Jason Brownlee, 2018, [Machine Learning Algorithms from Scratch with Python](https://machinelearningmastery.com/machine-learning-algorithms-from-scratch/). <br/>\n",
    "[2] Peter Harrington, 2012. Machine Learning in Action. Shelter Island, NY: Manning Publications Co."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
