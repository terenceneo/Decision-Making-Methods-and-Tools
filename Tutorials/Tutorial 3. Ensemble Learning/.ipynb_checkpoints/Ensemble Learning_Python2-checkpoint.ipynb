{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 Introduction to Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal\n",
    "\n",
    "In this notebook, we will explore **Ensemble Learning** including:\n",
    "* Bagging\n",
    "* Random Forest\n",
    "* AdaBoost\n",
    "\n",
    "For the **Decision Tree** method, you will:\n",
    "* Use open-source package to do ensemble learning\n",
    "* Compare performances of different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bias-Variance Decomposition\n",
    "\n",
    "\\begin{align}\n",
    "E[(y-\\hat{f}(x))^2] &= (E[\\hat{f}(x)-f(x)])^2 + E[\\hat{f}(x)-E[\\hat{f}(x)]]^2 + \\sigma^2  \\\\\n",
    "&= \\text{Bias}[\\hat{f}(x)]^2 + \\text{Var}[\\hat{f}(x)] + \\text{Irreducibel Error}\n",
    "\\end{align}\n",
    "\n",
    "A figure to illustrate **Bias** and **Variance**:\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/bias-and-variance.jpg\" width=\"400\">\n",
    "\n",
    "A chart to understand the tradeoff of **Bias** and **Variance**:\n",
    "<img src=\"http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png\" width=\"500\">\n",
    "\n",
    "An example to show **Overfit** problem: <br/>\n",
    "Which line is more preferred? Black or Green?\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/320px-Overfitting.svg.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Ensemble Learning\n",
    "Ensemble learning methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone ([Wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning)). \n",
    "\n",
    "Ensemble learning is an appropriate way to somehow \"relieve\" overfitting problem (Remember that ensemble learning methods may still overfit). From the lecture class, we know that a basic workflow of ensemble method:\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Nipaporn_Chanamarn/publication/308368870/figure/fig4/AS:408666126733315@1474445005322/The-concept-diagram-of-stacking-ensemble-learning-32.jpg \"Ensemble Learning\")\n",
    "\n",
    "Examples of Ensemble Learning methods include:\n",
    "<ol>\n",
    "    <li>Bootstrap Aggregating (aka.Bagging)</li>\n",
    "    <li>Random Forest</li>\n",
    "    <li>Boosting</li>    \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Examples\n",
    "\n",
    "### 3.1 Case on Handwritten Digit Recognition\n",
    "\n",
    "#### Dataset:\n",
    "\n",
    "The **Kaggle** competition dataset can be obtained from https://www.kaggle.com/c/digit-recognizer/data. \n",
    "\n",
    "#### Overview:\n",
    "\n",
    "MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of **handwritten images** has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "\n",
    "In this competition, your goal is to **correctly identify digits** from a dataset of tens of thousands of handwritten images. We’ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.\n",
    "\n",
    "#### Acknowlegements:\n",
    "\n",
    "More details about the dataset, including algorithms that have been tried on it and their levels of success, can be found at http://yann.lecun.com/exdb/mnist/index.html. The dataset is made available under a Creative Commons Attribution-Share Alike 3.0 license.\n",
    "\n",
    "#### Attributes:\n",
    "\n",
    "The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "\n",
    "The test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.\n",
    "\n",
    "The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset: You need to download dataset first\n",
    "%pwd\n",
    "train = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 42,000 pictures; Each picture is composed of 28*28 dimensional pixels\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What does an image look like\n",
    "plt.imshow(np.array(train.iloc[1,1:]).reshape((28, 28)), cmap=\"gray\")\n",
    "plt.title(\"This digit is %d\" % train.iloc[1,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a single decision tree model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mozartkun\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1162    0   16    8    9   15   22    0   12    4]\n",
      " [   0 1346   12    3    3    5    5    2   20    5]\n",
      " [  12   16 1045   40   17   18   22   22   35   12]\n",
      " [   5    5   43 1109    5   79    6   20   29   35]\n",
      " [   6    6   14    7 1001    9   19   11   26   83]\n",
      " [  21    7   12   56   14  926   38   11   44   26]\n",
      " [  27    6   19   12   34   34 1090    1   11   13]\n",
      " [   5   16   26    9   17    4    0 1170   10   44]\n",
      " [  12    8   25   46   28   45   23   14  998   29]\n",
      " [  15    7    8   31   72   27    5   47   31 1020]]\n",
      "Accuracy:  0.8624603174603175\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "# Tranform dataframe to array\n",
    "train_feature = train.iloc[:,1:].values\n",
    "train_target = train.iloc[:,0].values\n",
    "\n",
    "# Split train data: 70% for model fit, 30% for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_feature, train_target, test_size=0.3, random_state=12345)\n",
    "\n",
    "# Single decision tree model\n",
    "DT = DecisionTreeClassifier(criterion='entropy', random_state=12345)\n",
    "\n",
    "# Fit decision tree model\n",
    "DT_model = DT.fit(X_train, y_train)\n",
    "DT_model.classes_\n",
    "\n",
    "# Validation\n",
    "y_pred_valid = DT_model.predict(X_valid)\n",
    "\n",
    "# Performance of Random Forest model\n",
    "cm = confusion_matrix(y_valid, y_pred_valid)\n",
    "print cm\n",
    "print \"Accuracy: \", accuracy_score(y_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bagging\n",
    "\n",
    "A basic procedure for **Bagging** is:\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/python-deeper-insights/9781787128576/graphics/3547_07_06.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Aggregating Package\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Tranform dataframe to array\n",
    "train_feature = train.iloc[:,1:].values\n",
    "train_target = train.iloc[:,0].values\n",
    "\n",
    "# Split train data: 70% for model fit, 30% for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_feature, train_target, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Bagging Model; A bundle of decision trees\n",
    "# Just wait a minute...It will take some time\n",
    "BA = BaggingClassifier(n_estimators=100, random_state=12345)\n",
    "BA_model = BA.fit(X_train, y_train)\n",
    "BA_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "y_pred_valid = BA_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1215    0    6    0    3    5    6    1   11    1]\n",
      " [   0 1378   10    4    1    3    2    1    2    0]\n",
      " [  11    7 1160   12    6    1    7   15   16    4]\n",
      " [   5    3   27 1231    5   16    1   12   24   12]\n",
      " [   2    4    4    0 1111    2    6    5    7   41]\n",
      " [  15    3    2   20    4 1066   19    5   14    7]\n",
      " [   7    0    2    1    7   15 1201    2   12    0]\n",
      " [   1    6   17    1    3    0    1 1249    4   19]\n",
      " [   5    6    9   11    6   18   10    5 1135   23]\n",
      " [   7    4    8   18   26    8    1   11   12 1168]]\n",
      "Accuracy:  0.9455555555555556\n"
     ]
    }
   ],
   "source": [
    "# Performance of Random Forest model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_valid)\n",
    "print cm\n",
    "print \"Accuracy: \", accuracy_score(y_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest\n",
    "\n",
    "A basic procedure for **Random Forest** is:\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/ajTc5y3OqSQ/hqdefault.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest package\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Tranform dataframe to array\n",
    "train_feature = train.iloc[:,1:].values\n",
    "train_target = train.iloc[:,0].values\n",
    "\n",
    "# Split train data: 70% for model fit, 30% for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_feature, train_target, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Random Forest Model; Binary Splitting using Entropy\n",
    "RF = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=12345)\n",
    "RF_model = RF.fit(X_train, y_train)\n",
    "RF_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "y_pred_valid = RF_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1229    0    1    0    2    0    8    0    8    0]\n",
      " [   0 1385    5    4    3    1    1    0    2    0]\n",
      " [   6    3 1183    9    8    0   12   11    4    3]\n",
      " [   4    2   18 1260    1   18    0    9   15    9]\n",
      " [   2    4    2    0 1138    0    5    3    2   26]\n",
      " [   6    1    1   14    3 1104   12    1    9    4]\n",
      " [   6    1    0    0    5    9 1223    0    3    0]\n",
      " [   0    9   17    1    9    1    0 1244    4   16]\n",
      " [   3    4    7    8    6    7    9    1 1169   14]\n",
      " [   9    6    5   15   16    3    1   11    6 1191]]\n",
      "Accuracy:  0.9623809523809523\n"
     ]
    }
   ],
   "source": [
    "# Performance of Random Forest model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_valid)\n",
    "print cm\n",
    "print \"Accuracy: \", accuracy_score(y_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical procedure for boosting method is:\n",
    "\n",
    "<img src=\"https://koalaverse.github.io/machine-learning-in-R/images/boosting_diagram.png\" width=\"500\">\n",
    "\n",
    "Examples of boosting methods include:\n",
    "1. Boosting based on weights: Adaptive boosting (Adaboost)\n",
    "2. Boosting based on residuals: Gradient boosting decision trees (GBDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for **Adaboost** is:\n",
    "\n",
    "<img src=\"https://koalaverse.github.io/machine-learning-in-R/images/adaboost_m1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost package\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Tranform dataframe to array\n",
    "train_feature = train.iloc[:,1:].values\n",
    "train_target = train.iloc[:,0].values\n",
    "\n",
    "# Split train data: 70% for model fit, 30% for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_feature, train_target, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Adaboosting Model\n",
    "Ada = AdaBoostClassifier(n_estimators=100, random_state=12345)\n",
    "Ada_model = Ada.fit(X_train, y_train)\n",
    "Ada_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "y_pred_valid = Ada_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1024    1   23    0    0   84   97    0   14    5]\n",
      " [   0 1337   10   11    2   15    3    8   15    0]\n",
      " [  25   68  530   57   24   26  386   21   86   16]\n",
      " [  33   44   21  828   12   93   55   31  177   42]\n",
      " [   3   11   41    7  913   22   22   38   21  104]\n",
      " [  32   24   15  149   15  659   55    2  152   52]\n",
      " [   5   13   45   11    9   39 1103    3   18    1]\n",
      " [  33   14   30   10   21   26    0  953    6  208]\n",
      " [   9   59   27   71   15   58   26    8  906   49]\n",
      " [   7   11   64   24  260    7    2  114   29  745]]\n",
      "Accuracy:  0.7141269841269842\n"
     ]
    }
   ],
   "source": [
    "# Performance of Adaboosting model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_valid)\n",
    "print cm\n",
    "print \"Accuracy: \", accuracy_score(y_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about Ensemble Learning can be found at http://scikit-learn.org/stable/modules/ensemble.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose: Comparing single decision tree model with ensemble methods (5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "# Tranform dataframe to array\n",
    "train_feature = train.iloc[:,1:].values\n",
    "train_target = train.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Fold Cross Validation\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a single decision tree model**\n",
    "* Set `criterion='entropy'`, and `random_state=12345`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single decision tree model: 0.8485238095238095\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Single decision tree model\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy', random_state=12345)\n",
    "\n",
    "# Cross validation on Model 1\n",
    "cv_model_1 = cross_val_score(decision_tree, # Cross-validation on Model 1\n",
    "                             train.iloc[:,1:], # Feature matrix\n",
    "                             train.iloc[:,0], # Output vector\n",
    "                             cv=kf, # Cross-validation technique\n",
    "                             scoring='accuracy' # Model performance metrics: accuracy\n",
    "                            )\n",
    "\n",
    "# Report performance of Model 1\n",
    "print \"Single decision tree model: %s\" %(cv_model_1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a bagging model**\n",
    "* Set `n_estimators=100`, and `random_state=12345`\n",
    "* Reference: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging model: 0.9398333333333333\n"
     ]
    }
   ],
   "source": [
    "# Model 2: Bagging method\n",
    "bagging = BaggingClassifier(n_estimators=100, random_state=12345)\n",
    "\n",
    "# Cross validation on Model 2\n",
    "cv_model_2 = cross_val_score(bagging, # Cross-validation on Model 2\n",
    "                             train.iloc[:,1:], # Feature matrix\n",
    "                             train.iloc[:,0], # Output vector\n",
    "                             cv=kf, # Cross-validation technique\n",
    "                             scoring='accuracy' # Model performance metrics: accuracy\n",
    "                            )\n",
    "\n",
    "# Report performance of Model 2\n",
    "print \"Bagging model: %s\" %(cv_model_2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1. Create a random forest model (2 points)**\n",
    "* Set `criterion='entropy'`, `n_estimators=100`, and `random_state=12345`\n",
    "* Reference: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Random Forest method\n",
    "random_forest = \n",
    "\n",
    "# Cross validation on Model 3\n",
    "cv_model_3 = \n",
    "\n",
    "# Report performance of Model 3\n",
    "print \"Bagging model: %s\" %(cv_model_3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2. Create an Adaboost model (2 points)**\n",
    "* Set `n_estimators=100`, and `random_state=12345`\n",
    "* Reference: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: AdaBoosting method\n",
    "Adaboost = \n",
    "\n",
    "# Cross validation on Model 4\n",
    "cv_model_4 = \n",
    "\n",
    "# Report performance of Model 4\n",
    "print \"Bagging model: %s\" %(cv_model_4.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3. Based on your results, what is your finding? (1 point)**\n",
    "\n",
    "Hint: Just write your findings based on your model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 References\n",
    "\n",
    "[1] Chris Albon. (2018). Machine Learning with Python Cookbook. O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
